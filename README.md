# Bilibili_scraper
## 简介：

## 试验记录：

### 流程记录
![图1 七月四日之前的工作](https://github.com/kem190/Bilibili_scraper/blob/main/mindmap.png)
### **20240626**
关键字相关视频信息抓取，998条。存于jiangping_metadata2024_06_26.csv。以它们为seed，抓取了供4107条推荐视频，有19606条seed-related的相关关系。
### **20240628**
重写了相关视频抓取逻辑，用26号查询的关键视频重新抓取了相关视频。存于related_video_data_2.csv，这是后几日社会网络分析（SNA）和文本分析（CTA）的基础。
### **20240702**
终于靠b站教程完成了评论区翻页的抓取代码，抓取了seed视频的评论。这里犯的错误是限制了评论最大抓取数量到200条左右，导致相当数量的视频评论只有200条，而看不出评论变化的趋势。目前看来，在抓到评论的x条视频中，大部分只有少于十条评论，尔后评论数量开始均匀分布，直到200附近出现一个峰值。有理由怀疑后续也会均匀分布？或者评论数量干脆就是双峰分布的。
### **20240702**
痛定思痛，开始抓取全部4107条视频的评论（因为998条“似乎”只有标题包含姜萍这个关键字的），并且不设抓取上限。截至目前，程序已经跑了134593秒，仍然不知道它抓到了多少以及什么时候会停下。————我在写这个文档的时候突然发现了问题所在，这程序跑下来会比我想得长上百倍，故放弃。
### **20240703**
1. 对998-8031（6月26日的种子，7月2日的相关视频）的第一次相关视频数据，和4107-yy的第二次相关视频数据分别做了SNA，第一组数据分成明显的四个社区（见图2），第二组数据则非常散乱。
![图2 4107个视频的SNA分析](/jiangping_2_SNA.png)
1.1. 首先有趣的是时隔几天得到的related_vids 有巨大区别。这几个rel_vids 都标注了date of excecution，值得玩味一下。 （补充，这个不存在，看错了）<br />
1.2. 关于第一组数据分成的社区，还有待进一步探索，但在分析数据的过程中我发现我自己的bias，我还是太想看到“挺姜”和“反姜”了，这纯preconceived idea。当然，我也没找到这个。<br />
1.3. 目前需要的：对方法论内涵和外延的了解，内涵：它基于什么模型？我们做了哪些假设？外延：在什么场景下使用这种方法？能达到什么效果？<br />
   
3. 对998个跟姜萍最高相关度的视频下的评论做了WordFish估计，结果如图3。
![图3 对998个搜索得来的视频评论区的留言的WordFish估计](https://github.com/kem190/Bilibili_scraper/blob/main/jiangping_wordfish_community.jpeg)
2.1. 结果就是并没有得到我认为有意义的数据，还需要进一步的探索。这包含建立意义层面的理解和进一步的整理和把玩数据。比如做关键词清理、对视频的分类和筛选之类的。<br />
2.1.1. 假设要对视频进行筛选，我们可能需要一个明确的pipeline还有主旨。主旨是尽可能全、快，并且在单位时间内完整的收集数据并且完整的分析。在这个主旨下我选择了用“推荐视频”来做迭代的方式。所以说，选这种数据收集方式的原因是突出它的便利性，而不是我想把b站推送算法当作客体来分析。而如何筛选才能最终得到和主题相关的视频，或者说尽量全的得到视频，这我不得而知。<br />
3. 对998个最高相关度视频下的评论做了topic modelling，结果就是没产生有意义的结果（图4）。同时，用CaoJuan2009的方法来估计K值，结论是从5-50随着K增加结果越来越好，这不太真实。我知道不应该只依赖算法，我认为应该人为筛选一下相关数据。<br />
![图4 k=30时首当其冲的998个视频的主题建模结果](https://github.com/kem190/Bilibili_scraper/blob/main/jiangping_LDA_k%3D30.jpeg)
3.1. 主题建模同样没什么收获，并且不知道为什么。again，这需要一些对模型和方法的再认识。<br />

### **20240719**
以我目前对方法的理解，SNA不可避免地会引入b站推送算法的“倾向”。如果SNA做出两个组，然后我们能解读成挺姜和反姜，那也只是说明了算法的确有极化倾向（这也有文献支撑）。它能做的也只是帮我们把数据标注的工作自动化了，相当于做了unsupervised的分类。如果检测SNA处理后的组发现并非如此，这也只相当于p>0.05，没什么有意义的推测。我们可以用别的方法对数据进行探索式分析。因此，今天我打算对两个组分别进行主题建模，或者用wordfish的方式来看看它们是否的确不同。不过，在前两组内的，带有评论数据的视频似乎只有百分之四十（数据待check），这部分可能需要相当细碎的代码处理和一些manual labour。希望今天能做完。<br />
另外，对于新出炉（0704）的数据，也可以做一些描述性统计，比如看看评论分布之类的。事实上，应该用新出炉的数据的评论和之前的SNA结果匹配吧，来尝试一下。<br />

### **20240720**
从SNA给出的community出发，重做了topic modelling和Wordfish分析。这相当于有些自证，其中的逻辑是：“用b站推送情况自然形成的unsupervised的分组，就好像它是自然形成的一样”。<br />
1. Wordfish <br />
在论证游说的文章（）中，不同的企业分别为自己的组，在本研究中，不同的社区是自己的组。于是Wordfish就在试图说明这些社区的确有不同的潜在倾向，而如何解读这种倾向，也就是把beta的大小解读为什么，就要靠作者了。<br />
我试图从图5的词频分布上读出这种倾向：首先相比图3，这个图明显更平滑好看了，感觉是因为限制了占比共75%（683/909）的前19个组，导致数据量过小的outlier被排除了。看这张图，“姜萍”和“数学”处在中间，看到很让人安心，说明讨论的确是围绕着这两个主题进行的。而左侧有让人喜闻乐见的“主”和“圣”——基于知识背景我们知道这是反对姜萍的人会说的。同时左侧最远处有“姐妹”这时候再看右侧的词语，上面有“女性”所以我也很难判断这些人的成分。左侧还有一个突出的词汇是“反串”，也许正是一些反串式的评论会跟另一侧的人使用相同的词汇（比如姐妹），才导致做出这样混乱的数据。<br />
再看右侧，我很难认为这是挺姜的人，只看这些词语，似乎在讨论国内外的教材？在评价教育方式能否教出姜萍么？可能是一个更general的讨论，这个留给直接阅读评论或者主题分析的时候来做做看。总之，如果像这样分类，那么可以说左侧的更跟姜萍本人相关，讨论更针对事件本身，而右侧的更发散一些<br />
![图5 Wordfish给出的每个词的权重](https://github.com/kem190/Bilibili_scraper/blob/main/beta_distribution_of_words_19.jpeg)
从这种角度出发看图6的分组，就感觉组1和组2的人们在进行更大体的讨论，也难怪他们在SNA图的中间，联系起不同的组别。在看这些视频时，我们想更深入的了解事件本身，就通过推荐来到二3、4组的视频中。如果发散的看，可能就能在20条推荐中找到5的视频。这也许也说明了为什么在图2中，第五组处于核心地位，因为它的讨论更加广泛，不跟具体问题沾边。<br />
![图6 Wordfish赋分后分组的分布情况](https://github.com/kem190/Bilibili_scraper/blob/main/wordfish_results_of_19_communities.jpeg)
后面我只做了前六个组，一样是五个在一起，第五组在右侧的分布，也许这说明其他组获得的评论量已经不太影响模型生成了。
2. Topic modeling<br />
我将community作为文档的组别，重新做了主题建模。这次出来的结果也算有点好看。首先是用程序跑的主题数量，给出了预期的曲线（抱歉我没看懂CaoJuan2009的方法），我只知道k=8是好的（图7），暂时可用。于是用k=8跑了一下，出来的结果如图8。<br />
![图7 计算最佳的主题数的结果](https://github.com/kem190/Bilibili_scraper/blob/main/find_topic_number_result.jpeg)
![图8 8个主题的主题建模结果](https://github.com/kem190/Bilibili_scraper/blob/main/topics_k_8.jpeg)
主题一词语比较褒义，但出现奶茶我是不能理解的，在Wordfish中也有奶茶出现。主题2应该是围绕自证，但“诽谤”似乎说明了其偏向性。主题3，抓住了“集美”这个词汇，但从同组的“飞柱”可以看出这反而是讽刺女性的主题（当然这也设计主题建模的原理问题，它如何判断主题？如何关注组间和组内的词频？）。主题4，来到了教材问题，有很多人讨论，我猜测这就是我们采用的贴标签的方式（SNA）影响了数据的生成——这像是B站推到了一个内聚性很强的视频组中，这里面大家都在讨论教教材之类的问题，并且互相出现在对方的推送列表中。而我顺着SNA的标签分析，自然更凸显了这个主题的bias。主题五，反讽词语较多。主题6，这十个词很难说不是挺姜，至少讽刺性弱了很多。主题7、8，词语比较中立，看不懂。

当然，这里还是基于909个视频的评论数据做出的分析，事实上，这些视频并不是均匀的分布在每个组内的，这可能也对数据有一些影响。有些组的文本量明显严重大于其他组，不知道这会不会对模型估计产生影响。<br />

明天可以做的就是把新获得的评论数据带入来分析，这样应该有更多的视频来填满community，而不是一个community只有个别视频有数据。<br />
同时就是试图从其它的角度进行分析，比如IP地址、性别和时间。<br />
再有就是做一些数据上的描述，比如评论数量的分布等等，但不读文献找到方向，做这些也只是无头苍蝇乱撞了吧。<br />
睡前用了df_tokenized（而非之前一直用的comments_data_2）作为数据来源，数据（这两种分析的数据）一下好看了特别多，不知道为什么，明天看看吧。

### **20240722**
新数据太大了没法直接用R做wordfish，所以不得不想办法清理无关数据（当然优化程序可能也能做到就是暂时没这必要）。下面对今天做的处理进行报告：<br />
1. “所有”视频的统计与描述：<br />
首先，新数据是建立在related_vedios_0702的基础上进行收集的，也就是把直接搜索到的998条信息进行了一次相关视频迭代后的8096条视频作为基础。收集了他们的评论（见图1的流程图）。0702收集的种子，0705开始跑的，不重复的视频只有7230了（缺失的部分有公众号造成的重复（56条），还有删除的视频等）。总共抓取了2072347条评论。<br />
接下来用了xmc811的中国地图数据做了一下热力图。放上来了，如图9。但后续发现没什么太大意义，因为大部分视频都太general了，相比之下姜萍只占了百分之？？（虽然超过自然分布的占比很多但对数据而言不好看）。所以在现在做进一步的视频分析没什么实际意义，我们不得不真正给视频进行分类来得到更相关的数据，而不是现在这样的大杂烩。原git链接：(https://github.com/xmc811/mapchina)<br />
![图9 热力图](https://github.com/kem190/Bilibili_scraper/blob/main/%E8%A7%86%E9%A2%91%E8%AF%84%E8%AE%BA%E7%83%AD%E5%8A%9B%E5%9B%BE.jpeg)
性别方面，保密占大多数（1158686），填写性别的人中，男的有594724，填女的有170949条。至于本数据和网友的真实性别认同的偏差则需要其他文献支撑。<br />
由于这次收集了所有的评论，所以顺便做了评论数量分布，如图10：<br />
![图10 评论数量分布](https://github.com/kem190/Bilibili_scraper/blob/main/%E8%AF%84%E8%AE%BA%E6%95%B0%E9%87%8F%E5%88%86%E5%B8%83.jpeg)
3. 尝试分类。<br />
我希望之前的SNA能在分类上帮忙，我做了一个简单的尝试，把标题中的“姜萍”和“数学”这两个关键词的视频分离出来，然后通过看不同组中视频的减少量来判断SNA分析对于归类的作用，根据我7月20号的假设，组1组2跟姜萍更相关，组5较为不相关。所以，现在可以作出假设，组1，2的视频包含这两个关键词的数量会更多。然而事实并非如此，组1中有19.9%的视频标题包含这两个词，组2是19。1%，而组5是17.0%。无需做进一步的分析就可以发现他们差距并不大，因为前19组中，变化最少为13.6%，最多为24.6%。这种幅度的变化更多的说明了前面的假设都是白搞。姜萍的视频占比只有这么多，这应该属于比较正常的波动（当然也可以T检验？怎么检验一下）。<br />
基于此，我们不得不人工的筛选有效视频了。我选取了“主”“圣”这两个关键词，果然检索到了有意义的信息，进一步的证明了对context本身的理解对社科研究有重要意义。
![图10 评论数量分布](https://github.com/kem190/Bilibili_scraper/blob/main/%E4%B8%BB%E7%AD%89%E4%BA%8E6.png)
但我的知识会造成什么疏漏我也不知道，在没办法的情况下，人为浏览全部9000多个标题应该是最后的方法。实际上，我花在找自动化方法上的时间已经远超过了我自己做标注的时间，同时还得出很多错误结论。
通过姜萍和数学两个关键词筛选出来（实际上数学已经不完全是绝对相关的视频了）
5. 一些主观结论：<br />
大热视频中有一条《高等数学》的课程视频，的，评论数量37895，远高于第二名（10287，且第二名是“赌王”也几乎无关），所以如果直接去看edge list的话，可能很多视频最终都是跟他产生联系了，而不是我在今天之前认为的，和某个中立的或者有争议的姜萍视频产生联系。换言之，就算采用了我们这样的数据收集方式，得到跟主题相关的视频的效率也并不高（主题视频占比30左右）。
## 其他想法：
如果我把多次相关视频的seed-result联系起来，相当于人为施加权重。感觉不合理。但如果不这么相关，又能意味着什么呢？把程序当主体？<br />

### **20240723**
跟徐考之讨论之后确认了当下最重要的还是找到跟姜萍直接相关的内容，因为数据抓得太多实则有用信息很少。经过一些思考，我们排除了人为选择关键词的方案，因为这样难免添加bias。所以我打算只用姜萍二字作为关键词来进行筛选。<br />
首先标题直接出现姜萍二字的视频，有960条，这很直觉。接下来选择评论区中直接出现姜萍二字的视频，并且获取这些视频的其他评论。这样操作后发现，2076969条评论中，只有13698条直接提到这个词语的，共1312条视频，然而筛选“主”关键词的时候我并没有发现昨天发现的两个相关视频。进一步观察发现那个视频的20条评论中区的确没有姜萍二字，全都是“主主主主主”也够抽象的。总之，最终抓出349035条评论，1461条视频，有1244条评论是标题没有提及姜萍的视频中抓到的，这样一来，我们比最开始的抓的998条也是有所进步的，尽管这让我们做了8000条视频的操作。<br />
249095人贡献了349035条评论。地区、性别跟整体分布比没什么太大区别。<br />
当然，现在要做的很简单了，三板斧。SNA， Wordfish，topic models，开始。<br />

### **20240724**
深感进行不下去了，这时候可能需要的还是读更多的文献，而非在黑暗中瞎摸索。<br />
简单梳理一下方法上遇到的困境：<br />
首先是SNA，目前有两个想法，第一是在之前已经做好的SNA中找到这些目标视频并且把它们高亮出来。第二是在之前的edgelist里filter out不包含现在这些标题的视频，然后重做SNA。我怀疑后者的意义，可以做着玩玩倒是啦。<br />
第一种方法不是很合理，原本的互动包含了过多的外部元素，使得跑出来的模型不一定具有什么价值。所以直接用第二套思路来进行筛选。在原本记录互动的edgelist中筛选掉出发者和结束者都不在姜萍相关视频中的值，使得17168条互动变成了16475条……搞错啦，因为最开始的数据收集方式导致所有的种子都包含姜萍这个关键词hhh。所以应该用&而不是|，把两方都跟姜萍有关的筛选进来，这下变成6449条了。跑了SNA之后，组数也明显少了很多，从之前的150组变成了21组。接着我自己检查了一下这些分组，实际上作为人类我还是不太能理解其中的意义，所以说基本上用SNA给视频分类的方式宣告失败。<br />
接下来做了topic models，对此我首先遇到了划分文档的问题：之前是按community来划分，这几乎默认了b站推送和SNA这两者都有某种意义，但再观察后发现我们不应该如此认为。那么按照用户来划分文档显然过于细碎，但想按视频来划分则大概假设了同一个视频下的讨论大概是同质化的，这与经验不符。实际上，评论区的三条楼中楼可能能得出跟主评论完全相反的说法，或者至少是态度，这也许是topic models难以得出合理结果的原因。不过还是传个图看看吧，有很多杂乱的词汇，只是我不觉得清理了之后会有什么好处。也许清理一下完全重复的话（b站自动生成的那种）会好一些？或者说丢掉一些评论太少的视频。<br />
![图11 相关视频的k=12主题建模](https://github.com/kem190/Bilibili_scraper/blob/main/rel_topics_k_12.jpeg)
同样的问题在Wordfish中一样存在，对wordfish而言，如果将每个视频作为一个文档，那直接拟合不出模型，而用community一方面缺乏合法性，另一方面则是做不出有意义的数据。<br />
于是看了一眼去欧洲的机票，真贵啊。被生活击败的一天。

### **20240725**
随便做了一点初步的清理工作，发现确实值得做。所以说，之前不怎么做数据清理直接跑模型确实有点鲁莽了。现在一边清理一边记录一点好玩的事情吧。
需要清理数据的时候，我首先想到了b站自动生成的数据，什么“目测要火，先马一下”，所以我的想法是删除重复的评论，毕竟这种生成的数据都长得一样，然而评论应该是每个人自己的观点。结果我发现，有专门复制粘贴的评论，虽然他们是跟主题相关的，因为有些人会复制一些评论到处发。
清理了纯表情评论（'[doge]'这种的)：349405-345375。重复的评论仍然有49908条。有一些确实是有意义的，比如不同网友发的“坐等决赛”，有些则是不太有意义的，比如评论区养猫，或者评论区祝福。这评论区的猫还真挺可爱，但我也删不掉hhh。此后，又删除了空有一个at别人的，
打算采取粗暴一点的办法，删掉字数少于二的先看看。因为三个字的我看到了“主主主”。所以删掉了空有at别人的，来到了336978，又删除了中文字符少于两个的，到了320209。重复评论认同感有37032条。
继续检查这个重复评论，发现有一些评论似乎是at了其他视频，这种要保留么？保留吧，只把链接删掉。
后面发现哪怕视频只有一条评论，我们也很难说它没有质量，所以就这样吧，保留哪怕只有一条评论的视频。



## 注：
改代码确实有趣，但也确实效率低下。我因为脑子没在R和Python间切换过来，导致python中一处把 = 写成了 <- 而得不到正确的结果，这一个问题，我到处追查，花了一个多小时解决。<br />
